{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('../dataset/training.csv')  \n",
    "test = pd.read_csv('../dataset/test.csv')\n",
    "val = pd.read_csv('../dataset/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine data for consistent preprocessing\n",
    "# data = pd.concat([train, test, val], axis=0)\n",
    "# data = shuffle(data, random_state=42)\n",
    "\n",
    "# # Preprocessing function\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "#     text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))\n",
    "#     return text\n",
    "\n",
    "# data['cleaned_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  im feeling quite sad and sorry for myself but ...      0\n",
       "1  i feel like i am still looking at a blank canv...      0\n",
       "2                     i feel like a faithful servant      2\n",
       "3                  i am just feeling cranky and blue      3\n",
       "4  i can have for a treat or if i am feeling festive      1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split back into train, test, and validation\n",
    "# train = data.iloc[:len(train)]\n",
    "# test = data.iloc[len(train):len(train)+len(test)]\n",
    "# val = data.iloc[len(train)+len(test):]\n",
    "\n",
    "# Prepare TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_test = vectorizer.transform(test['text'])\n",
    "X_val = vectorizer.transform(val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "y_val = val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       586\n",
      "           1       0.85      0.96      0.90       686\n",
      "           2       0.88      0.69      0.77       154\n",
      "           3       0.87      0.77      0.82       276\n",
      "           4       0.88      0.79      0.83       228\n",
      "           5       0.89      0.56      0.68        70\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.78      0.82      2000\n",
      "weighted avg       0.87      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 1️⃣ Logistic Regression\n",
    "# # -----------------------------\n",
    "# lr = LogisticRegression(max_iter=200)\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_pred_lr = lr.predict(X_test)\n",
    "# print(\"Logistic Regression:\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       586\n",
      "           1       0.91      0.95      0.93       686\n",
      "           2       0.87      0.76      0.81       154\n",
      "           3       0.87      0.86      0.87       276\n",
      "           4       0.85      0.86      0.86       228\n",
      "           5       0.93      0.73      0.82        70\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.89      0.85      0.87      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 2️⃣ Support Vector Classifier\n",
    "# # -----------------------------\n",
    "# svc = SVC(kernel='linear', probability=True)\n",
    "# svc.fit(X_train, y_train)\n",
    "# y_pred_svc = svc.predict(X_test)\n",
    "# print(\"SVC:\\n\", classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       586\n",
      "           1       0.88      0.92      0.90       686\n",
      "           2       0.83      0.73      0.78       154\n",
      "           3       0.85      0.83      0.84       276\n",
      "           4       0.83      0.86      0.85       228\n",
      "           5       0.82      0.76      0.79        70\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.85      0.83      0.84      2000\n",
      "weighted avg       0.88      0.88      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 3️⃣ Random Forest\n",
    "# # -----------------------------\n",
    "# rf = RandomForestClassifier(n_estimators=100)\n",
    "# rf.fit(X_train, y_train)\n",
    "# y_pred_rf = rf.predict(X_test)\n",
    "# print(\"Random Forest:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/Development/BackAug/venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [01:22:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       586\n",
      "           1       0.88      0.92      0.90       686\n",
      "           2       0.79      0.81      0.80       154\n",
      "           3       0.86      0.86      0.86       276\n",
      "           4       0.86      0.86      0.86       228\n",
      "           5       0.73      0.83      0.78        70\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.85      0.86      0.85      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------\n",
    "# # 4️⃣ XGBoost\n",
    "# # -----------------------------\n",
    "# xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "# xgb.fit(X_train, y_train)\n",
    "# y_pred_xgb = xgb.predict(X_test)\n",
    "# print(\"XGBoost:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/Development/BackAug/venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 292ms/step - accuracy: 0.3287 - loss: 1.6135 - val_accuracy: 0.4595 - val_loss: 1.5208 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 339ms/step - accuracy: 0.5720 - loss: 1.1286 - val_accuracy: 0.7920 - val_loss: 0.5886 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - accuracy: 0.8422 - loss: 0.4415 - val_accuracy: 0.9110 - val_loss: 0.2677 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 355ms/step - accuracy: 0.9340 - loss: 0.2095 - val_accuracy: 0.9235 - val_loss: 0.2341 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - accuracy: 0.9537 - loss: 0.1395 - val_accuracy: 0.9245 - val_loss: 0.2408 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 337ms/step - accuracy: 0.9634 - loss: 0.1061 - val_accuracy: 0.9220 - val_loss: 0.2441 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 350ms/step - accuracy: 0.9696 - loss: 0.0791 - val_accuracy: 0.9245 - val_loss: 0.2410 - learning_rate: 5.0000e-04\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step\n",
      "ANN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       581\n",
      "           1       0.96      0.93      0.94       695\n",
      "           2       0.76      0.90      0.82       159\n",
      "           3       0.93      0.91      0.92       275\n",
      "           4       0.90      0.88      0.89       224\n",
      "           5       0.75      0.71      0.73        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.87      0.88      0.88      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val['text'])\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
    "\n",
    "ann = Sequential()\n",
    "ann.add(Embedding(max_words, 128, input_length=max_len))\n",
    "ann.add(SpatialDropout1D(0.2))\n",
    "ann.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "ann.add(BatchNormalization())\n",
    "ann.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "ann.add(Dense(64, activation='relu'))\n",
    "ann.add(Dropout(0.3))\n",
    "ann.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "ann.compile(loss='sparse_categorical_crossentropy', optimizer=AdamW(learning_rate=1e-3), metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = ann.fit(X_train_pad, y_train, epochs=20, batch_size=64, validation_data=(X_val_pad, y_val), callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "y_pred_ann = np.argmax(ann.predict(X_test_pad), axis=1)\n",
    "print(\"ANN:\\n\", classification_report(y_test, y_pred_ann))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a pickle file\n",
    "with open('../pickles/model.pkl', 'wb') as file:\n",
    "    pickle.dump(ann, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizer\n",
    "with open('../pickles/vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\n",
    "    \"i didnt feel humiliated\",\n",
    "    \"i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\",\n",
    "    \"im grabbing a minute to post i feel greedy wrong\",\n",
    "    \"i am ever feeling nostalgic about the fireplace i will know that it is still on the property\",\n",
    "    \"i am feeling grouchy\",\n",
    "    \"ive been feeling a little burdened lately wasnt sure why that was\",\n",
    "    \"ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny\",\n",
    "    \"i feel as confused about life as a teenager or as jaded as a year old man\",\n",
    "    \"i have been with petronas for years i feel that petronas has performed well and made a huge profit\",\n",
    "    \"i feel romantic too\",\n",
    "    \"i feel like i have to make the suffering i m seeing mean something\",\n",
    "    \"i do feel that running is a divine experience and that i can expect to have some type of spiritual encounter\",\n",
    "    \"i think it s the easiest time of year to feel dissatisfied\",\n",
    "    \"i feel low energy i m just thirsty\",\n",
    "    \"i have immense sympathy with the general point but as a possible proto writer trying to find time to write in the corners of life and with no sign of an agent let alone a publishing contract this feels a little precious\",\n",
    "    \"i do not feel reassured anxiety is on each side\",\n",
    "    \"i didnt really feel that embarrassed\",\n",
    "    \"i feel pretty pathetic most of the time\",\n",
    "    \"i started feeling sentimental about dolls i had as a child and so began a collection of vintage barbie dolls from the sixties\",\n",
    "    \"i now feel compromised and skeptical of the value of every unit of work i put in\",\n",
    "    \"i feel irritated and rejected without anyone doing anything or saying anything\",\n",
    "    \"i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for\",\n",
    "    \"i have the feeling she was amused and delighted\",\n",
    "    \"i was able to help chai lifeline with your support and encouragement is a great feeling and i am so glad you were able to help me\",\n",
    "    \"i already feel like i fucked up though because i dont usually eat at all in the morning\",\n",
    "    \"i still love my so and wish the best for him i can no longer tolerate the effect that bm has on our lives and the fact that is has turned my so into a bitter angry person who is not always particularly kind\",\n",
    "    \"i feel so inhibited in someone elses kitchen like im painting on someone elses picture\",\n",
    "    \"i become overwhelmed and feel defeated\",\n",
    "    \"i feel kinda appalled that she feels like she needs to explain in wide and lenghth her body measures etc pp\",\n",
    "    \"i feel more superior dead chicken or grieving child\",\n",
    "    \"i get giddy over feeling elegant in a perfectly fitted pencil skirt\",\n",
    "    \"i remember feeling acutely distressed for a few days\",\n",
    "    \"i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies\",\n",
    "    \"i climbed the hill feeling frustrated that id pretty much paced entirely wrong for this course and that a factor that has never ever hampered me had made such a dent in the day\",\n",
    "    \"i can t imagine a real life scenario where i would be emotionally connected enough with someone to feel totally accepted and safe where it it morally acceptable for me to have close and prolonged physical contact and where sex won t be expected subsequently\",\n",
    "    \"i am not sure what would make me feel content if anything\",\n",
    "    \"i have been feeling the need to be creative\",\n",
    "    \"i do however want you to know that if something someone is causing you to feel less then your splendid self step away from them\",\n",
    "    \"i feel a bit rude writing to an elderly gentleman to ask for gifts because i feel a bit greedy but what is christmas about if not mild greed\",\n",
    "    \"i need you i need someone i need to be protected and feel safe i am small now i find myself in a season of no words\",\n",
    "    \"i plan to share my everyday life stories traveling adventures inspirations and handmade creations with you and hope you will also feel inspired\",\n",
    "    \"i already have my christmas trees up i got two and am feeling festive which i m sure is spurring me to get started on this book\",\n",
    "    \"ive worn it once on its own with a little concealer and for the days im feeling brave but dont want to be pale then its perfect\",\n",
    "    \"i feel very strongly passionate about when some jerk off decides to poke and make fun of us\",\n",
    "    \"i was feeling so discouraged we are already robbing peter to pay paul to get our cow this year but we cant afford to not get the cow this way\",\n",
    "    \"i was feeling listless from the need of new things something different\",\n",
    "    \"i lost my special mind but don t worry i m still sane i just wanted you to feel what i felt while reading this book i don t know how many times it was said that sam was special but i can guarantee you it was many more times than what i used in that paragraph did i tell you she was special\",\n",
    "    \"i can t let go of that sad feeling that i want to be accepted here in this first home of mine\",\n",
    "    \"on a boat trip to denmark\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[2]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[5]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[4]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[2]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[4]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[4]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[4]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[5]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[3]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[2]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for text in arr:\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_len)\n",
    "    predicted_class = np.argmax(ann.predict(text_pad), axis=1)\n",
    "    print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
